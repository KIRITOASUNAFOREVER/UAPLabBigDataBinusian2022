{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Etonic.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vw5EbtZmCD4C"
      },
      "source": [
        "from pyspark.sql import SparkSession\r\n",
        "\r\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0Ts8V5GCcWj"
      },
      "source": [
        "training = spark.read.option(\"inferSchema\",\"true\").csv(\"Planet_Training.csv\",header=True)\r\n",
        "testing = spark.read.option(\"inferSchema\",\"true\").csv(\"Planet_Testing.csv\",header=True)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eGPdP8pCm7k"
      },
      "source": [
        "training = training.select(\"Temperature\",\"Water\",\"Atmosphere Color\",\"Habitable\")\r\n",
        "testing = testing.select(\"Temperature\",\"Water\",\"Atmosphere Color\",\"Habitable\")"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvh4dydtDTo-"
      },
      "source": [
        "training = training.na.drop()\r\n",
        "testing = testing.na.drop()"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMLK3FjxDVms"
      },
      "source": [
        "from pyspark.sql.functions import when\r\n",
        "\r\n",
        "training = training.withColumn(\"Water\",when(training[\"Water\"] == \"Low\",0)\r\n",
        "                                                .when(training[\"Water\"] == \"Medium\",1)\r\n",
        "                                                .otherwise(2))\r\n",
        "\r\n",
        "testing = testing.withColumn(\"Water\",when(testing[\"Water\"] == \"Low\",0)\r\n",
        "                                                .when(testing[\"Water\"] == \"Medium\",1)\r\n",
        "                                                .otherwise(2))\r\n",
        "\r\n",
        "training = training.withColumn(\"Atmosphere Color\",when(training[\"Atmosphere Color\"] == \"Red\",0)\r\n",
        "                                                .when(training[\"Atmosphere Color\"] == \"Blue\",1)\r\n",
        "                                                .otherwise(2))\r\n",
        "\r\n",
        "testing = testing.withColumn(\"Atmosphere Color\",when(testing[\"Atmosphere Color\"] == \"Red\",0)\r\n",
        "                                                .when(testing[\"Atmosphere Color\"] == \"Blue\",1)\r\n",
        "                                                .otherwise(2))"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBlA9rbLFDYc"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\r\n",
        "\r\n",
        "cols = training.columns\r\n",
        "cols.remove(\"Habitable\")\r\n",
        "\r\n",
        "training = VectorAssembler(inputCols=cols, outputCol=\"Features\").transform(training)\r\n",
        "testing = VectorAssembler(inputCols=cols, outputCol=\"Features\").transform(testing)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMUvTysOFKFi"
      },
      "source": [
        "scaler = StandardScaler(inputCol=\"Features\",outputCol=\"Scaled Features\")\r\n",
        "training = scaler.fit(training).transform(training)\r\n",
        "testing = scaler.fit(testing).transform(testing)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cNx9H3dFNBi"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\r\n",
        "\r\n",
        "model = LogisticRegression(featuresCol=\"Scaled Features\",labelCol=\"Habitable\",maxIter=10).fit(training)\r\n",
        "prediction = model.transform(testing)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYMy5C-SFTcz",
        "outputId": "bb8b7a85-e9e4-48ba-e16d-b7410e22a8be"
      },
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\r\n",
        "\r\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",labelCol=\"Habitable\")\r\n",
        "result = evaluator.evaluate(prediction)\r\n",
        "print(\"Accuracy: {}%\".format(result * 100))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 91.71043337232418%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}